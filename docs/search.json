[
  {
    "objectID": "jacks_car_rental.html",
    "href": "jacks_car_rental.html",
    "title": "MDPs and Dynamic Programming",
    "section": "",
    "text": "1 Introduction\n  2 Markov Decision Processes\n  3 Jack‚Äôs Car Rental\n  \n  3.1 Implementation\n  \n  3.1.1 Poisson Distribution\n  3.1.2 Configuration\n  3.1.3 States and Actions\n  3.1.4 Policy Iteration"
  },
  {
    "objectID": "jacks_car_rental.html#implementation",
    "href": "jacks_car_rental.html#implementation",
    "title": "MDPs and Dynamic Programming",
    "section": "3.1 Implementation",
    "text": "3.1 Implementation\n\n3.1.1 Poisson Distribution\nBefore we get started with modeling the MDP, it will help us to have a class to represent arbitrary Poisson distributions. In particular, we care about the probability mass function (PMF) which tells us the probability of observing a particular outcome of the random event modeled by the distribution.\nThe PMF of a Poisson is defined as\n\\[\nP(N=n ; \\lambda) = \\frac{\\lambda^n}{n!}e^{-\\lambda}\n\\]\n\nimport math\nimport numpy as np\n\n\ndef poisson_pmf(n: int, mu: float):\n    \"\"\"The Poisson probability mass function.\n\n    Args:\n        n (int): the outcome of the random event.\n        mu (float): the expected value of the random event.\n    \"\"\"\n    return (mu**n) / (math.factorial(n)) * math.e ** (-mu)\n\n\nclass Poisson:\n    def __init__(self, mu: float, upper_n: int = 15):\n        \"\"\"Initialize a new Poisson distribution.\n\n        This class caches values from the PMF for all values\n        of n in the inclusive range [0, upper_n].\n\n        Args:\n            mu (float): the mean parameter.\n            upper_n (int): the largest outcome we care about to observe.\n        \"\"\"\n        assert mu &gt; 0, \"mu must be a positive number\"\n        assert upper_n &gt;= 0, \"n must be a non-negative integer\"\n        self.mu = mu\n        self.upper_n = upper_n\n        self.pmf = np.zeros(upper_n + 1)\n\n        for n in range(upper_n + 1):\n            # rely on scipy for PMF implementation\n            self.pmf[n] = poisson_pmf(n, self.mu)\n\n        # if we capture PMF for events [0, upper_n], we will approach\n        # a sum of 1 as we increase upper_n. the excluded tail contains\n        # some mass, so we redistribute it to the included mass\n        self.pmf /= self.pmf.sum()\n\nHaving the PMF values precomputed and stored in NumPy arrays will help us to write more efficient, vectorized code later on when we implement policy iteration.\n\n\n3.1.2 Configuration\nThis section is pretty self-explanatory: we will capture the relevant configuration parameters for the problem and store them in constants.\n\n# max number of cars at either location at once\nMAX_CARS = 20\n\n# max number of cars that can be moved during the night\nMAX_MOVES = 5\n\n# reward (cost) of moving a single car\nMOVE_COST = -2\n\n# reward of renting a car\nRENT_CREDIT = 10\n\n# MDP discount factor\nGAMMA = 0.9\n\n# poisson parameters\nPOIS_REQ_MU = (3, 4)  # expected daily requests at location (1, 2)\nPOIS_RET_MU = (3, 2)  # expected daily returns at location (1, 2)\n\n# lets also instantiate distributions for these parameters\n# (take advantage of the class we created earlier)\nPOIS_REQ = tuple(map(Poisson, POIS_REQ_MU))\nPOIS_RET = tuple(map(Poisson, POIS_RET_MU))\n\n\n\n3.1.3 States and Actions\nLet‚Äôs define the state and action types that we will use. I choose \\(s\\in\\cal{S}\\) to be a tuple of integers representing the count of cars at each location, and \\(a\\in\\cal{A}(s)\\) to be an integer representing the count of cars moved to the second location.\nThis representation of \\(\\cal{A}(s)\\) allows us to represent a flow of cars from the first location to the second location as a positive integer, and a flow of cars from the second location to the first as a negative integer. We can choose this representation because we only care about the total flow between locations. If we moved 4 cars from location 1 to location 2, and 1 car from location 2 to location 1, we‚Äôd have a net movement of 3 cars to location 2. This is the same as moving 3 cars from location 1 to location 2 and 0 cars from location 2 to location 1, but is cheaper (we move 2 less cars!) so will always be preferred.\n\nfrom typing import Sequence\n\nState = tuple[int, int]\n\"\"\"\nThe MDP state is an (i, j) pair of the number of cars at each location.\n\"\"\"\n\nAction = int\n\"\"\"\nThe MDP action is an integer indicating the movement of cars.\n\nA positive value indicates a flow from the first location to the second.\nA negative value indicates a flow from the second location to the first.\nA value of zero indicates no movement of cars.\n\"\"\"\n\n\ndef states() -&gt; Sequence[State]:\n    \"\"\"Return a collection of all possible states.\n\n    Returns:\n        Sequence[State]: a list of all possible (i, j) state pairs.\n    \"\"\"\n    return [(i, j) for i in range(0, MAX_CARS + 1) for j in range(0, MAX_CARS + 1)]\n\n\ndef actions(state: State) -&gt; Sequence[Action]:\n    \"\"\"Return a collection of all possible actions from the given state.\n\n    Args:\n        state (State): the starting state.\n\n    Returns:\n        Sequence[Action]: a list of all possible (and valid) actions.\n    \"\"\"\n    # the total number of cars moveable from either location is bounded by:\n    #   - the MAX_MOVES parameter\n    #   - the number of cars at the source location\n    #   - the number of spots at the target location\n    upper_bound = min(MAX_MOVES, state[0], (MAX_CARS - state[1]))\n    lower_bound = min(MAX_MOVES, state[1], (MAX_CARS - state[0]))\n    return range(-lower_bound, upper_bound + 1)\n\nI also define two helper functions to return the state set \\(\\cal{S}\\) and action sets \\(\\cal{A}(s)\\) for convenience. The action(state: State) function is particularly useful because it ensures that we only iterate over valid actions during policy improvement.\n\n\n3.1.4 Policy Iteration\nNow we‚Äôre getting to the good stuff! Policy iteration, as explained by Sutton and Barto (2018), is an iterative loop between two stages: policy evaluation and policy improvement.\nWe start out with an arbitrary value function \\(V_0\\) and policy \\(\\pi_0\\) (note: we use an uppercase \\(V\\) here instead of a lowercase \\(v\\) because \\(V\\) is a tabular approximation of the function \\(v\\)). First, we run policy evaluation to update \\(V\\) such that it converges to the true value function for the current policy:\n\\[\nV_{i+1} \\approx v_{\\pi_{i}}\n\\]\nThen, we run policy improvement to update \\(\\pi\\) such that it is greedy with respect to the current value function:\n\\[\n\\pi_{i+1}(s) = \\text{argmax}_a \\sum_{s',r} p(s',r|s,a)[r + \\gamma V_i(s')] \\;\\;\\forall s \\in \\cal{S}\n\\]\nThrough cycling back and forth between these two stages, we approach the optimal solution for our MDP."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Finite MDPs and Dynamic Programming (RL Part 1)",
    "section": "",
    "text": "I recently stumbled upon two papers in deep reinforcement learning (DRL): Ha and Schmidhuber (2018) and Hafner et al. (2024), which have resparked my enthusiasm for deep learning, reinforcement learning (RL), and their combination. To capitalize on this, I acquired my own copy of Reinforcement Learning: An Introduction by Sutton and Barto (2018) to read through. The last time I read it was for an undergraduate RL class, and I felt that I needed to brush up on my foundational RL knowledge, diving deep into core theory and intuition, before I jumped into learning more about DRL. I also want to explore doing research in DRL, so it is even more important that I solidify my core competencies.\nTo help me learn more effectively and practice my own writing skill (which I believe is critical for successful communication in life), I thought it would be a nice idea to jot notes about key takeaways I found in the book and write about them. I also thought it would be both fun and productive to implement some of the example problems/challenges in real code so that I could reinforce the concepts explored in the book and play around with them. That‚Äôs what this article is about. I write a little bit about Chapters 3 and 4 from the text and also include a full solution to the Jack‚Äôs Car Rental problem using policy iteration and value iteration, two forms of dynamic programming.\nI hope this article serves as a good resource for others to learn from; it certainly did for me as I was creating it. üòä"
  },
  {
    "objectID": "index.html#policies-and-value-functions",
    "href": "index.html#policies-and-value-functions",
    "title": "Finite MDPs and Dynamic Programming (RL Part 1)",
    "section": "Policies and Value Functions",
    "text": "Policies and Value Functions\nWhen we are discussing the decision-making ability of an agent, we are referring to that agent‚Äôs policy. A policy is a mapping of states to a probability distribution that represents the chances of selecting an action \\(a\\) from a given state \\(s\\). We write the policy as \\(\\pi(a|s)\\), which represents the probability of taking action \\(a\\) from state \\(s\\). In the case that an agent‚Äôs policy is deterministic as opposed to stochastic, we can simply write \\(\\pi(s)\\).\nIn order to select an optimal policy \\(\\pi_*\\), we must first quantify the quality of policies with respect to their expected returns. This can be accomplished through value functions and action-value functions. The value function \\(v_\\pi(s)\\) quantifies how good it is for an agent to be in a specific state, and the action-value function \\(q_\\pi(s,a)\\) quantifies how good it is for an agent to be in a specific state and select a specific action. Because the return of a trajectory is governed by the policy an agent follows, the value function and action-value function are also defined with respect to a policy. They are defined as follows:\n\\[\nv_\\pi(s_t) = \\mathbb{E}_\\pi\\lbrack G_t | S_t = s_t \\rbrack\n\\tag{1}\\]\n\\[\nq_\\pi(s_t, a_t) = \\mathbb{E}_\\pi\\lbrack G_t | S_t = s_t, A_t = a_t \\rbrack\n\\tag{2}\\]"
  },
  {
    "objectID": "index.html#bellman-equations",
    "href": "index.html#bellman-equations",
    "title": "Finite MDPs and Dynamic Programming (RL Part 1)",
    "section": "Bellman Equations",
    "text": "Bellman Equations\nFor any policy \\(\\pi\\), the value function \\(v_\\pi\\) has a recursive property in that for any state \\(s\\), the value function can be defined with respect to its successor states \\(s' \\in \\cal{S}\\):\n\\[\nv_\\pi(s)\n=\n\\sum_{a}\\pi(a|s)\n\\sum_{s',r}p(s',r|s, a)\\lbrack r + \\gamma v_\\pi(s') \\rbrack\n\\tag{3}\\]\nThis equation is referred to as the Bellman equation for the value function, and the corresponding Bellman equation for the action-value function is:\n\\[\nq_\\pi(s, a)\n=\n\\sum_{s',r}p(s',r|s, a)\\lbrack r + \\gamma \\sum_{a'}\\pi(a'|s')q_\\pi(s',a') \\rbrack\n\\tag{4}\\]\nThe function \\(p(s',r|s,a)\\) represents the probability of transitioning to the next state \\(s'\\) and receiving a reward \\(r\\) given that you start in state \\(s\\) and take action \\(a\\). This function is known as the dynamics of the MDP.\nBoth Equation¬†3 and Equation¬†4 are both derived from Equation¬†1 and Equation¬†2, and the full steps can be found in the text (although they are short and sweet üòä)."
  },
  {
    "objectID": "index.html#optimal-policies-and-value-functions",
    "href": "index.html#optimal-policies-and-value-functions",
    "title": "Finite MDPs and Dynamic Programming (RL Part 1)",
    "section": "Optimal Policies and Value Functions",
    "text": "Optimal Policies and Value Functions\nFor finite MDPs, because the state and action spaces are finite, there is a finite number of policies that exist. If we quantify the ‚Äúscore‚Äù of each policy \\(\\pi\\) by its value function \\(v_\\pi(s)\\), then it follows that there must exist at least one policy \\(\\pi_*\\) such that \\(v_{*}(s)\\geq v_\\pi(s)\\) for all other \\(\\pi\\) and for each \\(s \\in \\cal{S}\\). We call this an optimal policy because an agent maximizes their expected return when adhering to it.\nThe optimal Bellman equations for the value function and action-value function are subtly different from Equation¬†3 and Equation¬†4; they are defined as follows:\n\\[\nv_*(s)\n=\n\\max_{a}\n\\sum_{s',r}p(s',r|s, a)\\lbrack r + \\gamma v_*(s') \\rbrack\n\\tag{5}\\]\n\\[\nq_*(s, a)\n=\n\\sum_{s',r}p(s',r|s, a)\\lbrack r + \\gamma \\max_{a'}q_*(s',a') \\rbrack\n\\tag{6}\\]"
  },
  {
    "objectID": "index.html#policy-evaluation",
    "href": "index.html#policy-evaluation",
    "title": "Finite MDPs and Dynamic Programming (RL Part 1)",
    "section": "Policy Evaluation",
    "text": "Policy Evaluation\nPolicy evaluation enables us to compute the value function \\(v_\\pi(s)\\) for an arbitrary policy \\(\\pi\\). In iterative policy evaluation, we take Equation¬†3 and turn it into an update rule to iteratively approximate \\(v_\\pi(s)\\):\n\\[\nv_{k+1}(s)\n=\n\\sum_{a}\\pi(a|s)\n\\sum_{s',r}p(s',r|s, a)\\lbrack r + \\gamma v_k(s') \\rbrack\n\\tag{7}\\]\nAt each iteration, the value \\(v_{k+1}(s)\\) is assigned to the expectation of the immediate reward conditioned on the policy, and utilizing the previous approximation of the value function \\(v_k(s)\\) at each successor state \\(s'\\). After enough iterations, this process converges.\nThis is called an expected update because it averages out all possible actions rather than relying on a single action from sample data. This is in contrast to other methods further in the text such as Monte Carlo.\nThe algorithm presented for iterative policy evaluation is as follows (implemented in Python):\ndef policy_evaluation(V, pi, theta: float = 0.1):\n    while True:\n        delta = 0\n        for s in iter_states():\n            v = V[s]\n            V[s] = expected_return(s, a)\n            delta = max(delta, abs(v - V[s]))\n        if delta &lt; theta:\n            break\nwhere iter_states() is a helper function to iterate over all possible states, and expected_return(s: State, a: Action) implements Equation¬†3 to return the value of state s and action a.\nOnce we have converged to a final value function for the given policy, we may find that the policy is not optimal (in other words, there exists a state such that the policy action does not have the highest expected return). In this case, we may wish to improve our policy such that it is more optimal."
  },
  {
    "objectID": "index.html#policy-improvement",
    "href": "index.html#policy-improvement",
    "title": "Finite MDPs and Dynamic Programming (RL Part 1)",
    "section": "Policy Improvement",
    "text": "Policy Improvement\nPolicy improvement is the reverse of policy iteration. We start with a value function \\(v_\\pi(s)\\), and we want to improve our policy such that it is optimal with respect to the given value function. One such way is to just pick a greedy policy that maximizes the expected action-value, given to us by \\(v_\\pi(s)\\). The following equation gives us the optimal policy that is greedy with respect to the current value function:\n\\[\n\\pi'(s)=\\underset{a}{\\text{argmax}} \\sum_{s',r} p(s',r|s,a)[r+\\gamma v_\\pi(s')]\n\\]\nThis is the version if we used \\(q_\\pi(s,a)\\):\n\\[\n\\pi'(s)=\\underset{a}{\\text{argmax}}\\; q_\\pi(s,a)\n\\]\nAnd here it is implemented in Python code:\ndef policy_improvement(V, pi):\n    policy_stable = True\n    for s in iter_states():\n        # store old (action,value) pair\n        old_a = pi[s]\n        old_v = expected_return(s, pi[s])\n        # create accumulator to find max (action,value) pair\n        best_a, best_v = pi[s], old_v\n        for a in iter_actions(a):\n            if a == old_a:\n                # skip checking the old action, it is our baseline\n                continue\n            v = expected_return(s, a)\n            if v &gt; best_v:\n                best_a, best_v = a, v\n        pi[s] = best_a\n        if pi[s] != old_a:\n            policy_stable = False\n    return policy_stable"
  },
  {
    "objectID": "index.html#policy-iteration",
    "href": "index.html#policy-iteration",
    "title": "Finite MDPs and Dynamic Programming (RL Part 1)",
    "section": "Policy Iteration",
    "text": "Policy Iteration\nPolicy iteration is the process of iteratively cycling back between policy evaluation and policy improvement until we converge to an optimal policy. In finite MDPs this procedure is guaranteed to converge, as each step of policy evaluation and improvement produces a policy more optimal than the last. We stop policy iteration when policy improvement does not change the policy, i.e.¬†it is already greedy w.r.t. the value function."
  },
  {
    "objectID": "index.html#value-iteration",
    "href": "index.html#value-iteration",
    "title": "Finite MDPs and Dynamic Programming (RL Part 1)",
    "section": "Value Iteration",
    "text": "Value Iteration\nValue iteration is similar to policy iteration, except it omits entirely the process of policy improvement. We adjust our policy evaluation method slightly such that we do not rely on any policy \\(\\pi\\). During each iteration over the states, we compute the value function update for each possible action \\(a\\) rather than just one action selected from our policy, and we pick the highest value. This essentially combines policy evaluation and improvement into one sweep since we are updating our value function in a greedy fashion (selecting action with maximum expected return)."
  },
  {
    "objectID": "index.html#poisson-distribution",
    "href": "index.html#poisson-distribution",
    "title": "Finite MDPs and Dynamic Programming (RL Part 1)",
    "section": "Poisson Distribution",
    "text": "Poisson Distribution\nBefore we implement the MDP, it will help us to have a class to represent arbitrary Poisson distributions. In particular, we care about the probability mass function (PMF) which tells us the probability of observing a particular outcome of the random event modeled by the distribution.\nThe PMF of a Poisson is defined as\n\\[\nP(N=n ; \\lambda) = \\frac{\\lambda^n}{n!}e^{-\\lambda}\n\\]\n\nimport math\nimport numpy as np\n\n\ndef poisson_pmf(n: int, mu: float):\n    \"\"\"The Poisson probability mass function.\n\n    Args:\n        n (int): the outcome of the random event.\n        mu (float): the expected value of the random event.\n    \"\"\"\n    return (mu**n) / (math.factorial(n)) * math.e ** (-mu)\n\n\nclass Poisson:\n    def __init__(self, mu: float, upper_n: int = 15):\n        \"\"\"Initialize a new Poisson distribution.\n\n        This class caches values from the PMF for all values\n        of n in the inclusive range [0, upper_n].\n\n        Args:\n            mu (float): the mean parameter.\n            upper_n (int): the largest outcome we care about to observe.\n        \"\"\"\n        assert mu &gt; 0, \"mu must be a positive number\"\n        assert upper_n &gt;= 0, \"n must be a non-negative integer\"\n        self.mu = mu\n        self.upper_n = upper_n\n        self.pmf = np.zeros(upper_n + 1)\n\n        for n in range(upper_n + 1):\n            # rely on scipy for PMF implementation\n            self.pmf[n] = poisson_pmf(n, self.mu)\n\n        # if we capture PMF for events [0, upper_n], we will approach\n        # a sum of 1 as we increase upper_n. the excluded tail contains\n        # some mass, so we redistribute it to the included mass\n        self.pmf /= self.pmf.sum()\n\nHaving the PMF values precomputed and stored in NumPy arrays will help us to write more efficient, vectorized code later on when we implement policy iteration."
  },
  {
    "objectID": "index.html#mdp-formalization",
    "href": "index.html#mdp-formalization",
    "title": "Finite MDPs and Dynamic Programming (RL Part 1)",
    "section": "MDP Formalization",
    "text": "MDP Formalization\nNext, we can capture relevant parameters from the problem statement as constants in our code and write some helper functions to interact with the state and action spaces of our MDP.\n\nfrom typing import Sequence\n\n\n# max number of cars at either location at once\nMAX_CARS = 20\n\n# max number of cars that can be moved during the night\nMAX_MOVES = 5\n\n# reward (cost) of moving a single car\nMOVE_COST = -2\n\n# reward of renting a car\nRENT_CREDIT = 10\n\n# MDP discount factor\nGAMMA = 0.9\n\n# poisson parameters\nPOIS_REQ_MU = (3, 4)  # expected daily requests at location (1, 2)\nPOIS_RET_MU = (3, 2)  # expected daily returns at location (1, 2)\n\n# lets also instantiate distributions for these parameters\n# (take advantage of the class we created earlier)\nUPPER_N = 15\nPOIS_REQ = tuple(map(lambda mu: Poisson(mu, upper_n=UPPER_N), POIS_REQ_MU))\nPOIS_RET = tuple(map(lambda mu: Poisson(mu, upper_n=UPPER_N), POIS_RET_MU))\n\nState = tuple[int, int]\n\"\"\"\nThe MDP state is an (i, j) pair of the number of cars at each location.\n\"\"\"\n\nAction = int\n\"\"\"\nThe MDP action is an integer indicating the movement of cars.\n\nA positive value indicates a flow from the first location to the second.\nA negative value indicates a flow from the second location to the first.\nA value of zero indicates no movement of cars.\n\"\"\"\n\n\ndef states() -&gt; Sequence[State]:\n    \"\"\"Return a collection of all possible states.\n\n    Returns:\n        Sequence[State]: a list of all possible (i, j) state pairs.\n    \"\"\"\n    return [(i, j) for i in range(0, MAX_CARS + 1) for j in range(0, MAX_CARS + 1)]\n\n\ndef actions(state: State) -&gt; Sequence[Action]:\n    \"\"\"Return a collection of all possible actions from the given state.\n\n    Args:\n        state (State): the starting state.\n\n    Returns:\n        Sequence[Action]: a list of all possible (and valid) actions.\n    \"\"\"\n    # the total number of cars moveable from either location is bounded by:\n    #   - the MAX_MOVES parameter\n    #   - the number of cars at the source location\n    #   - the number of spots at the target location\n    upper_bound = min(MAX_MOVES, state[0], (MAX_CARS - state[1]))\n    lower_bound = min(MAX_MOVES, state[1], (MAX_CARS - state[0]))\n    return range(-lower_bound, upper_bound + 1)"
  },
  {
    "objectID": "index.html#dynamics",
    "href": "index.html#dynamics",
    "title": "Finite MDPs and Dynamic Programming (RL Part 1)",
    "section": "Dynamics",
    "text": "Dynamics\nThe next step is to model the dynamics of the world. From reading the original problem statement above, we know that the environment functions in the following way:\n\nAt the end of day \\(t-1\\), Jack selects an action \\(a \\in [-5, 5]\\), equivalent to the number of cars moving between the first and second locations before day \\(t\\).\nJack is issued a negative reward of \\(-2\\cdot|a|\\) for moving the cars\nWe sample four independent random variables from four Poisson distributions defined by the scale parameters \\(\\lambda\\) of the problem statement (\\(3\\) and \\(4\\) for requests, and \\(3\\) and \\(2\\) for returns at each location respectively).\nJack is issued a positive reward of \\(10\\cdot\\min(\\text{\\# cars}, \\text{\\# requests})\\)\nWe subtract the number of requests from the count of cars at each location, and add the number of returns, maxing sure that no count goes above 20 (in this case, cars return to the nationwide company lot).\nWait for Jack‚Äôs next action.\n\nWhen implementing the dynamics in Python, we can take advantage of the fact that the joint probability of our four random variables will never change, so we can compute it once.\n\n# generate all possible observations of four random variables.\nobservations = np.array(\n    [\n        (requests_1, requests_2, returns_1, returns_2)\n        for requests_1 in range(UPPER_N + 1)\n        for requests_2 in range(UPPER_N + 1)\n        for returns_1 in range(UPPER_N + 1)\n        for returns_2 in range(UPPER_N + 1)\n    ]\n).T\n\n# get the probability of each joint observation occuring:\n# - product of four independent probabilities\njoint_probability = np.array(\n    [\n        POIS_REQ[0].pmf.take(observations[0]),\n        POIS_REQ[1].pmf.take(observations[1]),\n        POIS_RET[0].pmf.take(observations[2]),\n        POIS_RET[1].pmf.take(observations[3]),\n    ]\n).prod(axis=0)\n\nprint(f\"Shape of observations:\", observations.shape)\nprint(f\"Shape of joint_probability:\", joint_probability.shape)\n\nShape of observations: (4, 65536)\nShape of joint_probability: (65536,)"
  },
  {
    "objectID": "index.html#policy-iteration-1",
    "href": "index.html#policy-iteration-1",
    "title": "Finite MDPs and Dynamic Programming (RL Part 1)",
    "section": "Policy Iteration",
    "text": "Policy Iteration\nHere I define a class to hold the individual components of policy iteration, and cycle between them.\n\nclass PolicyIteration:\n    def __init__(self):\n        self.pi = np.zeros((MAX_CARS + 1, MAX_CARS + 1), dtype=int)\n        self.V = np.zeros((MAX_CARS + 1, MAX_CARS + 1), dtype=float)\n\n    def expected_return(self, state: State, action: Action):\n        # assign cost for moving cars, this is deterministic\n        move_reward = MOVE_COST * abs(action)\n\n        # get the state of cars after the nightly moves\n        state_night = (state[0] - action, state[1] + action)\n\n        # get the number of orders that are fulfilled\n        #  - observations[0] = requests at location 1\n        #  - observations[1] = requests at location 2\n        # we are summing over all possible next states (s',r|s,a)\n        # so we use prior NumPy arrays to help speed up computation\n        fulfilled = (\n            np.minimum(state_night[0], observations[0]),\n            np.minimum(state_night[1], observations[1]),\n        )\n\n        # get the next state after all fulfilled rental requests are\n        # removed, and returned cars are added back\n        #  - observations[2] = returns at location 1\n        #  - observations[3] = returns at location 2\n        state_next = (\n            np.minimum(state_night[0] - fulfilled[0] + observations[2], MAX_CARS),\n            np.minimum(state_night[1] - fulfilled[1] + observations[3], MAX_CARS),\n        )\n\n        # get reward for each fulfilled rental request\n        rent_reward = RENT_CREDIT * (fulfilled[0] + fulfilled[1])\n\n        # dot product will compute weighed sum of possible stochastic returns\n        # - weighing is joint probability from prior calculation\n        expected = move_reward + joint_probability @ (\n            rent_reward + GAMMA * V[state_next]\n        )\n\n        return expected\n\n    def policy_evaluation(self, theta: float = 0.1):\n        \"\"\"Run policy evaluation.\"\"\"\n        while True:\n            delta = 0\n            for state in states():\n                v = self.V[state]\n                # note: asynchronous update\n                self.V[state] = self.expected_return(state, self.pi[state])\n                delta = max(delta, abs(v - self.V[state]))\n            if delta &lt; theta:\n                return\n\n    def policy_improvement(self) -&gt; bool:\n        \"\"\"Run policy improvement.\"\"\"\n        policy_stable = True\n        for state in states():\n            best_action, best_v = self.pi[state], self.V[state]\n            for action in actions(state):\n                if action == self.pi[state]:\n                    continue\n                v = self.expected_return(state, action)\n                if v &gt; best_v:\n                    best_action, best_v = action, v\n            if best_action != self.pi[state]:\n                policy_stable = False\n                self.pi[state] = best_action\n        return policy_stable\n\n    def solve(self, max_iter: int = 5):\n        \"\"\"Run policy iteration.\"\"\"\n        yield self.pi, self.V\n        for i in range(1, max_iter + 1):\n            self.policy_evaluation()\n            policy_stable = self.policy_improvement()\n            yield self.pi, self.V\n            if policy_stable:\n                return"
  },
  {
    "objectID": "index.html#results",
    "href": "index.html#results",
    "title": "Finite MDPs and Dynamic Programming (RL Part 1)",
    "section": "Results",
    "text": "Results\n\nimport matplotlib.pyplot as plt\n\n\npolicy_iteration = PolicyIteration()\nfor i, (pi, V) in enumerate(policy_iteration.solve()):\n\n    fig, axes = plt.subplots(ncols=2, figsize=(10, 4))\n\n    # Plot value function V\n    im0 = axes[0].imshow(V, cmap=\"viridis\")\n    axes[0].invert_yaxis()\n    axes[0].set_title(rf\"$V_{{{i}}}$\")\n    axes[0].set_xlabel(\"Number of cars at location 1\")\n    axes[0].set_ylabel(\"Number of cars at location 2\", rotation=90)\n    nx, ny = V.shape[1], V.shape[0]\n    axes[0].set_xticks(np.arange(0, nx, 5))\n    axes[0].set_yticks(np.arange(0, ny, 5))\n    fig.colorbar(im0, ax=axes[0], orientation=\"vertical\", fraction=0.046, pad=0.04)\n\n    # Plot policy pi\n    im1 = axes[1].imshow(pi, cmap=\"viridis\")\n    axes[1].invert_yaxis()\n    axes[1].set_title(rf\"$\\pi_{{{i}}}$\")\n    axes[1].set_xlabel(\"Number of cars at location 1\")\n    axes[1].set_ylabel(\"Number of cars at location 2\", rotation=90)\n    nx2, ny2 = pi.shape[1], pi.shape[0]\n    axes[1].set_xticks(np.arange(0, nx2, 5))\n    axes[1].set_yticks(np.arange(0, ny2, 5))\n    fig.colorbar(im1, ax=axes[1], orientation=\"vertical\", fraction=0.046, pad=0.04)\n\n    plt.tight_layout()\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIt looks like we successfully matched the plots from the text!"
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "Finite MDPs and Dynamic Programming (RL Part 1)",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWhen working with incomplete/partial state knowledge, we refer to Partially observable Markov decision processes (or POMDPs for short).‚Ü©Ô∏é\nWe denote the action as \\(\\cal{A}(s)\\) as opposed to \\(\\cal{A}\\) to indicate that the set of actions is dependent upon the current state. For example, we will consider a maze. The state is your position in the maze, and the action is whether you move north, south, east, or west. If you reach a dead end in a maze, that direction will not be available to you, so the set of actions available to you depends on where you are in the maze. When the same set of actions is available at every state, we can use the latter representation as shorthand.‚Ü©Ô∏é"
  }
]